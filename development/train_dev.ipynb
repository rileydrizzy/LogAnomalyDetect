{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-09 04:10:07.508074: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-09 04:10:07.542095: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-09 04:10:07.543260: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-09 04:10:08.560092: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from time import strftime\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils.utils import (get_dataset, get_tokenizer, set_seed, tensorboard,\n",
    "                         tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/gitpod/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"doc\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import hydra\n",
    "import nltk\n",
    "import polars as pl\n",
    "from nltk.corpus import stopwords\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "from utils.logger import logger\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "\n",
    "def clean_text(text_row):\n",
    "    \"\"\"performs preprocessing steps on each text row removing numbers,\n",
    "    stopwords, punctuation and any symbols\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    clean_text : row\n",
    "        A cleaned and preprocessed text\n",
    "    \"\"\"\n",
    "\n",
    "    text_row = text_row.lower()\n",
    "    text_row = re.sub(\"<[^>]*>\", \"\", text_row)\n",
    "    text_row = re.sub(r\"[^a-zA-Z\\s]\", \"\", text_row)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    text_row = [\n",
    "        word\n",
    "        for word in text_row.split()\n",
    "        if word not in stop_words and word not in string.punctuation\n",
    "    ]\n",
    "    text_cleaned = \" \".join(word for word in text_row)\n",
    "    return text_cleaned\n",
    "\n",
    "\n",
    "def label_encoder(target_df):\n",
    "    \"\"\"performs label encoding for target label\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    label : int\n",
    "        return either 0 for normal or 1 for abnormal\n",
    "    \"\"\"\n",
    "\n",
    "    if target_df == \"normal\":\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "    return label\n",
    "\n",
    "\n",
    "def preprocess_and_encode(file_path, save_path):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : _type_\n",
    "        _description_\n",
    "    save_path : _type_\n",
    "        _description_\n",
    "    \"\"\"\n",
    "    dataframe = pl.read_parquet(file_path)\n",
    "    dataframe = dataframe.with_columns(\n",
    "        pl.col(\"Target\").apply(label_encoder, return_dtype=pl.Int32)\n",
    "    )\n",
    "    dataframe = dataframe.with_columns(pl.col(\"Log\").apply(clean_text))\n",
    "    dataframe.write_parquet(file=save_path, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/log_anomaly'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 42\n"
     ]
    }
   ],
   "source": [
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "optim = tf.keras.optimizers.Adadelta(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import GRU\n",
    "\n",
    "model_name ='New_Testing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'development/dev.gzip'\n",
    "\n",
    "preprocess_and_encode(data_path, 'development/clean_test.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(2,), dtype=string, numpy=\n",
      "array([b'rmncju rmncju ras kernel info ce sym xef mask x',\n",
      "       b'rmnccju rmnccju ras kernel info generating core'], dtype=object)>, <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 0], dtype=int32)>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-09 04:10:10.876700: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "clean_data = 'development/clean_test.gzip'\n",
    "dataset = get_dataset(file_path=clean_data,shuffle= True)\n",
    "for sample in dataset.take(1):\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer, vocab_size = get_tokenizer(dataset)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return tokenizer(text), label\n",
    "final_dataset = dataset.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(2, 20), dtype=int64, numpy=\n",
      "array([[  2,   2,   3,   4,   5,  29,  30, 271,  28,  23,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0],\n",
      "       [  2,   2,   3,   4,   5,   7,   6,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0]])>, <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 0], dtype=int32)>)\n"
     ]
    }
   ],
   "source": [
    "for sample in final_dataset.take(1):\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRU.build_model(embed_dim=10, vocab_size=vocab_size,pad= 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = tensorboard(model_name)\n",
    "model.compile(loss=loss, optimizer=optim, metrics= ['f1_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'tensorflow.python.data.ops.map_op._MapDataset'>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mfit(final_dataset, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[tensorboard_cb])\n",
      "File \u001b[0;32m/workspace/log_anomaly/env/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/workspace/log_anomaly/env/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1766\u001b[0m, in \u001b[0;36mtrain_validation_split\u001b[0;34m(arrays, validation_split)\u001b[0m\n\u001b[1;32m   1764\u001b[0m unsplitable \u001b[39m=\u001b[39m [\u001b[39mtype\u001b[39m(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m flat_arrays \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _can_split(t)]\n\u001b[1;32m   1765\u001b[0m \u001b[39mif\u001b[39;00m unsplitable:\n\u001b[0;32m-> 1766\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1767\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`validation_split` is only supported for Tensors or NumPy \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1768\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39marrays, found following types in the input: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(unsplitable)\n\u001b[1;32m   1769\u001b[0m     )\n\u001b[1;32m   1771\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(t \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m flat_arrays):\n\u001b[1;32m   1772\u001b[0m     \u001b[39mreturn\u001b[39;00m arrays, arrays\n",
      "\u001b[0;31mValueError\u001b[0m: `validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'tensorflow.python.data.ops.map_op._MapDataset'>]"
     ]
    }
   ],
   "source": [
    "model.fit(final_dataset, validation_split=0.2, callbacks=[tensorboard_cb])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

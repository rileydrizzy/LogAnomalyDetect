{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import polars as pl\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from functools import partial\n",
    "\n",
    "def clean_text_preprocess(text_row):\n",
    "    \"\"\" performs preprocessing steps on each text row removing numbers, \n",
    "    stopwords, punctuation and any symbols\n",
    "\n",
    "    \n",
    "    Returns \n",
    "    -------\n",
    "    clean_text : row\n",
    "        A cleaned and preprocessed text \n",
    "    \"\"\"\n",
    "\n",
    "    text_row = text_row.lower()\n",
    "    text_row = re.sub('<[^>]*>', '', text_row)\n",
    "    text_row = re.sub(r'[^a-zA-Z\\s]', '', text_row)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_row = [word for word in text_row.split()\n",
    "            if word not in stop_words and word not in string.punctuation]\n",
    "    clean_text = ' '.join(word for word in text_row)\n",
    "    return clean_text\n",
    "\n",
    "def label_encoder(target_df):\n",
    "    \"\"\"performs label encoding for target label \n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    label : int\n",
    "        return either 0 for normal or 1 for abnormal\n",
    "    \"\"\"\n",
    "\n",
    "    if target_df == 'normal':\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "    return label\n",
    "\n",
    "def get_dataset(file_path, batch_size, shuffle_size= 10, shuffle = True):\n",
    "    \"\"\"create a Tensorflow dataset, with shuffle, batching and prefetching activated\n",
    "    to speed up computation during training\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        path of the parquet file\n",
    "    batch_size : int\n",
    "        Batch size \n",
    "    shuffle_size : int\n",
    "        Size of the buffer for shuffle \n",
    "    shuffle : bool, Default = True\n",
    "        perform shuffle on the dataset, if false it doesn't\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataset : Dataset\n",
    "        A tensorflow Dataset with features and label\n",
    "    \"\"\"\n",
    "\n",
    "    dataframe = pl.read_parquet(file_path)\n",
    "    dataframe = dataframe.with_columns(pl.col('Target').apply(\n",
    "            label_encoder, return_dtype=pl.Int32))\n",
    "    dataframe = dataframe.with_columns(pl.col('Log').apply(\n",
    "            clean_text_preprocess))\n",
    "    features_df = dataframe['Log']\n",
    "    target_df = dataframe['Target']\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features_df, target_df))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(shuffle_size)\n",
    "    dataset = dataset.batch(batch_size).prefetch(buffer_size= tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'dev.gzip'\n",
    "dev_df = pl.read_parquet(file)\n",
    "dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = get_dataset(file_path= file, batch_size=1,shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in dev_dataset.take(2):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_ds = dev_dataset.map(lambda text,label: text)\n",
    "sequence_length = 10\n",
    "tokenizer_layer = tf.keras.layers.TextVectorization(split= 'whitespace', output_mode= 'int',\n",
    "                                              output_sequence_length= sequence_length)\n",
    "tokenizer_layer.adapt(log_ds)\n",
    "vocab_size = tokenizer_layer.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = 10\n",
    "kernel = 5\n",
    "stride = 1\n",
    "pad= \"same\"\n",
    "embed_dim = 100\n",
    "\n",
    "def build_model():\n",
    "    \"\"\" 1DCNN doc\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : object\n",
    "        model\n",
    "    \"\"\"\n",
    "    Inpput_layer = tf.keras.layers.Input(shape= ())\n",
    "    embeding_layer = tf.keras.layers.Embedding(input_dim=vocab_size + 1,output_dim= embed_dim,\n",
    "                                               mask_zero= True)\n",
    "    DefaultConv1D = partial(tf.keras.layers.Conv1D, kernel_size= 3, strides= 1 , padding= pad, \n",
    "                           activation= 'relu')\n",
    "    DefualtMaxpool1D = partial(tf.keras.layers.MaxPool1D, pool_size= 2)\n",
    "    optim = tf.keras.optimizers.Adam(learning_rate= 0.01)\n",
    "\n",
    "    model = tf.keras.Sequential([embeding_layer,\n",
    "                         DefaultConv1\n",
    "                         'D(30),\n",
    "                         DefualtMaxpool1D(),\n",
    "                         tf.keras.layers.GlobalMaxPool1D(),\n",
    "                         tf.keras.layers.Dropout(0.5),\n",
    "                         tf.keras.layers.Dense(units= 20, activation= 'relu'),\n",
    "                         tf.keras.layers.Dropout(0.5),\n",
    "                         tf.keras.layers.Dense(units= 1, activation= 'sigmoid')\n",
    "                         ])\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer= optim, metrics= ['f1_score'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "log-anaomly-Z6xKN3XP-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

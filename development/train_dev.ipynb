{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from time import strftime\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Main Workspace\\\\LogAnomalyDetect\\\\development'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Main Workspace\\LogAnomalyDetect\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_dataset, preprocess_and_encode, get_vectorization_layer\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_device_strategy\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_seed\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dataset'"
     ]
    }
   ],
   "source": [
    "from src.dataset.data_loader import get_dataset, preprocess_and_encode, get_vectorization_layer\n",
    "from src.utils.common_utils import get_device_strategy\n",
    "from src.utils.common_utils import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'set_seed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mset_seed\u001b[49m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'set_seed' is not defined"
     ]
    }
   ],
   "source": [
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'development/dev.gzip'\n",
    "clean_dev = \"development/clean_dev.gzip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Log</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1119803499 2005.06.26 R03-M0-NC-C:J03-U01 200...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1119803105 2005.06.26 R04-M0-NE-C:J06-U11 200...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1121496169 2005.07.15 R06-M0-N6-C:J03-U01 200...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1120968564 2005.07.09 R26-M0-N0-C:J03-U01 200...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1120953205 2005.07.09 R27-M0-N7-C:J11-U01 200...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Log  Target\n",
       "0   1119803499 2005.06.26 R03-M0-NC-C:J03-U01 200...  normal\n",
       "1   1119803105 2005.06.26 R04-M0-NE-C:J06-U11 200...  normal\n",
       "2   1121496169 2005.07.15 R06-M0-N6-C:J03-U01 200...  normal\n",
       "3   1120968564 2005.07.09 R26-M0-N0-C:J03-U01 200...  normal\n",
       "4   1120953205 2005.07.09 R27-M0-N7-C:J11-U01 200...  normal"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(data_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess_and_encode(data_path, \"development/clean_dev.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = get_dataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(2,), dtype=string, numpy=\n",
      "array([b' 1119803499 2005.06.26 R03-M0-NC-C:J03-U01 2005-06-26-09.31.39.485965 R03-M0-NC-C:J03-U01 RAS KERNEL INFO data store interrupt caused by dcbf.........0\\n',\n",
      "       b' 1119803105 2005.06.26 R04-M0-NE-C:J06-U11 2005-06-26-09.25.05.389021 R04-M0-NE-C:J06-U11 RAS KERNEL INFO program interrupt: illegal instruction......1\\n'],\n",
      "      dtype=object)>, <tf.Tensor: shape=(2,), dtype=string, numpy=array([b'normal', b'normal'], dtype=object)>)\n",
      "(<tf.Tensor: shape=(2,), dtype=string, numpy=\n",
      "array([b' 1121496169 2005.07.15 R06-M0-N6-C:J03-U01 2005-07-15-23.42.49.989372 R06-M0-N6-C:J03-U01 RAS KERNEL INFO generating core.1735\\n',\n",
      "       b' 1120968564 2005.07.09 R26-M0-N0-C:J03-U01 2005-07-09-21.09.24.789209 R26-M0-N0-C:J03-U01 RAS KERNEL INFO generating core.16239\\n'],\n",
      "      dtype=object)>, <tf.Tensor: shape=(2,), dtype=string, numpy=array([b'normal', b'normal'], dtype=object)>)\n",
      "(<tf.Tensor: shape=(2,), dtype=string, numpy=\n",
      "array([b' 1120953205 2005.07.09 R27-M0-N7-C:J11-U01 2005-07-09-16.53.25.993596 R27-M0-N7-C:J11-U01 RAS KERNEL INFO generating core.9065\\n',\n",
      "       b' 1117942121 2005.06.04 R32-M1-N5-C:J10-U01 2005-06-04-20.28.41.514519 R32-M1-N5-C:J10-U01 RAS KERNEL INFO CE sym 9, at 0x045ba220, mask 0x02\\n'],\n",
      "      dtype=object)>, <tf.Tensor: shape=(2,), dtype=string, numpy=array([b'normal', b'normal'], dtype=object)>)\n"
     ]
    }
   ],
   "source": [
    "for idx in df_dataset.take(3):\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dataset = get_dataset(clean_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(2,), dtype=string, numpy=\n",
      "array([b'rmnccju rmnccju ras kernel info data store interrupt caused dcbf',\n",
      "       b'rmnecju rmnecju ras kernel info program interrupt illegal instruction'],\n",
      "      dtype=object)>, <tf.Tensor: shape=(2,), dtype=int16, numpy=array([0, 0], dtype=int16)>)\n",
      "(<tf.Tensor: shape=(2,), dtype=string, numpy=\n",
      "array([b'rmncju rmncju ras kernel info generating core',\n",
      "       b'rmncju rmncju ras kernel info generating core'], dtype=object)>, <tf.Tensor: shape=(2,), dtype=int16, numpy=array([0, 0], dtype=int16)>)\n"
     ]
    }
   ],
   "source": [
    "for idx in clean_dataset.take(2):\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_layer, vocab_s = get_vectorization_layer(clean_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = token_layer(\"k e r n e l i n f o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=int64, numpy=\n",
       "array([  1, 151, 291, 310, 151, 184,   1, 310, 162,   1,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0], dtype=int64)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = get_device_strategy()\n",
    "S.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.model_loader import ModelLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = ModelLoader()\n",
    "test_model_loader = loader.get_model(\"cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = test_model_loader(embedding_vocab= vocab_s, embedding_dim= 5,\n",
    "                        vectorization_layer=token_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "optim = tf.keras.optimizers.Adadelta(learning_rate=0.01)\n",
    "tensorb = tf.keras.callbacks.TensorBoard()\n",
    "f1_score = tf.keras.metrics.F1Score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.compile(loss=loss, optimizer=optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 6s 9ms/step - loss: 0.6863 - val_loss: 0.6790\n",
      "Epoch 2/20\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.6715 - val_loss: 0.6627\n",
      "Epoch 3/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.6536 - val_loss: 0.6418\n",
      "Epoch 4/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.6304 - val_loss: 0.6179\n",
      "Epoch 5/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.6084 - val_loss: 0.5926\n",
      "Epoch 6/20\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.5784 - val_loss: 0.5634\n",
      "Epoch 7/20\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.5538 - val_loss: 0.5329\n",
      "Epoch 8/20\n",
      "500/500 [==============================] - 3s 6ms/step - loss: 0.5198 - val_loss: 0.4995\n",
      "Epoch 9/20\n",
      "500/500 [==============================] - 3s 6ms/step - loss: 0.4875 - val_loss: 0.4642\n",
      "Epoch 10/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.4586 - val_loss: 0.4288\n",
      "Epoch 11/20\n",
      "500/500 [==============================] - 3s 6ms/step - loss: 0.4300 - val_loss: 0.3940\n",
      "Epoch 12/20\n",
      "500/500 [==============================] - 4s 9ms/step - loss: 0.3902 - val_loss: 0.3593\n",
      "Epoch 13/20\n",
      "500/500 [==============================] - 3s 6ms/step - loss: 0.3676 - val_loss: 0.3279\n",
      "Epoch 14/20\n",
      "500/500 [==============================] - 3s 6ms/step - loss: 0.3465 - val_loss: 0.3010\n",
      "Epoch 15/20\n",
      "500/500 [==============================] - 3s 6ms/step - loss: 0.3201 - val_loss: 0.2765\n",
      "Epoch 16/20\n",
      "500/500 [==============================] - 3s 7ms/step - loss: 0.3088 - val_loss: 0.2555\n",
      "Epoch 17/20\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.2840 - val_loss: 0.2389\n",
      "Epoch 18/20\n",
      "500/500 [==============================] - 4s 9ms/step - loss: 0.2773 - val_loss: 0.2236\n",
      "Epoch 19/20\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.2587 - val_loss: 0.2123\n",
      "Epoch 20/20\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.2585 - val_loss: 0.2023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2b62b504110>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.fit(clean_dataset,epochs=20, validation_data= clean_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "500/500 [==============================] - 10s 15ms/step - loss: 0.6919 - val_loss: 0.6830\n",
      "Epoch 2/2\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.6743 - val_loss: 0.6653\n"
     ]
    }
   ],
   "source": [
    "with S.scope():\n",
    "    token_layer, vocab_s = get_vectorization_layer(clean_dataset)\n",
    "    new_test_model = test_model_loader(embedding_vocab= vocab_s, embedding_dim= 5,\n",
    "                        vectorization_layer=token_layer)\n",
    "    new_test_model.compile(loss=loss, optimizer=optim)\n",
    "    new_test_model.fit(clean_dataset,epochs=2, validation_data= clean_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(2, 20), dtype=int64, numpy=\n",
      "array([[  2,   2,   3,   4,   5,  29,  30, 271,  28,  23,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0],\n",
      "       [  2,   2,   3,   4,   5,   7,   6,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0]])>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return tokenizer(text), label\n",
    "final_dataset = dataset.map(vectorize_text)\n",
    "for sample in final_dataset.take(1):\n",
    "    print(sample)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

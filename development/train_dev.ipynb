{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 16:49:30.276494: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-10 16:49:30.310442: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-10 16:49:30.311089: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-10 16:49:31.108614: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from time import strftime\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils.utils import (get_dataset, get_tokenizer, set_seed, tensorboard,\n",
    "                         tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/gitpod/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"doc\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import hydra\n",
    "import nltk\n",
    "import polars as pl\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "\n",
    "def clean_text(text_row):\n",
    "    \"\"\"performs preprocessing steps on each text row removing numbers,\n",
    "    stopwords, punctuation and any symbols\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    clean_text : row\n",
    "        A cleaned and preprocessed text\n",
    "    \"\"\"\n",
    "\n",
    "    text_row = text_row.lower()\n",
    "    text_row = re.sub(\"<[^>]*>\", \"\", text_row)\n",
    "    text_row = re.sub(r\"[^a-zA-Z\\s]\", \"\", text_row)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    text_row = [\n",
    "        word\n",
    "        for word in text_row.split()\n",
    "        if word not in stop_words and word not in string.punctuation\n",
    "    ]\n",
    "    text_cleaned = \" \".join(word for word in text_row)\n",
    "    return text_cleaned\n",
    "\n",
    "\n",
    "def label_encoder(target_df):\n",
    "    \"\"\"performs label encoding for target label\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    label : int\n",
    "        return either 0 for normal or 1 for abnormal\n",
    "    \"\"\"\n",
    "\n",
    "    if target_df == \"normal\":\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "    return label\n",
    "\n",
    "\n",
    "def preprocess_and_encode(file_path, save_path):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : _type_\n",
    "        _description_\n",
    "    save_path : _type_\n",
    "        _description_\n",
    "    \"\"\"\n",
    "    dataframe = pl.read_parquet(file_path)\n",
    "    dataframe = dataframe.with_columns(\n",
    "        pl.col(\"Target\").apply(label_encoder, return_dtype=pl.Int32)\n",
    "    )\n",
    "    dataframe = dataframe.with_columns(pl.col(\"Log\").apply(clean_text))\n",
    "    dataframe.write_parquet(file=save_path, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/log_anomaly'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 42\n"
     ]
    }
   ],
   "source": [
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "optim = tf.keras.optimizers.Adadelta(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import GRU\n",
    "\n",
    "model_name ='New_Testing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'development/dev.gzip'\n",
    "\n",
    "preprocess_and_encode(data_path, 'development/clean_test.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = 'development/clean_test.gzip'\n",
    "df = pl.read_parquet(clean_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Log</th><th>Target</th></tr><tr><td>str</td><td>i32</td></tr></thead><tbody><tr><td>&quot;rmnccju rmnccj…</td><td>0</td></tr><tr><td>&quot;rmnecju rmnecj…</td><td>0</td></tr><tr><td>&quot;rmncju rmncju …</td><td>0</td></tr><tr><td>&quot;rmncju rmncju …</td><td>0</td></tr><tr><td>&quot;rmncju rmncju …</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "┌───────────────────────────────────┬────────┐\n",
       "│ Log                               ┆ Target │\n",
       "│ ---                               ┆ ---    │\n",
       "│ str                               ┆ i32    │\n",
       "╞═══════════════════════════════════╪════════╡\n",
       "│ rmnccju rmnccju ras kernel info … ┆ 0      │\n",
       "│ rmnecju rmnecju ras kernel info … ┆ 0      │\n",
       "│ rmncju rmncju ras kernel info ge… ┆ 0      │\n",
       "│ rmncju rmncju ras kernel info ge… ┆ 0      │\n",
       "│ rmncju rmncju ras kernel info ge… ┆ 0      │\n",
       "└───────────────────────────────────┴────────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(2,), dtype=string, numpy=\n",
      "array([b'rmncju rmncju ras kernel info ce sym xef mask x',\n",
      "       b'rmnccju rmnccju ras kernel info generating core'], dtype=object)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 16:49:32.793735: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "clean_data = 'development/clean_test.gzip'\n",
    "dataset = get_dataset(file_path=clean_data,shuffle= True)\n",
    "for sample in dataset.take(1):\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer, vocab_size = get_tokenizer(dataset)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embed_dim, Sequnce_length):\n",
    "    \"\"\"1DCNN doc\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : object\n",
    "        model\n",
    "    \"\"\"\n",
    "\n",
    "    #input_ = tf.keras.layers.Input(shape=(Sequnce_length))\n",
    "    embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size+ 1, \n",
    "                                                output_dim=embed_dim, mask_zero= True)\n",
    "    conv1D = tf.keras.layers.Conv1D(filters=10, kernel_size= 2)\n",
    "    pool = tf.keras.layers.MaxPool1D()\n",
    "    flatten = tf.keras.layers.GlobalAveragePooling1D()\n",
    "    drop1 = tf.keras.layers.Dropout(0.5)\n",
    "    dense_layer = tf.keras.layers.Dense(units =100, activation='relu')\n",
    "    drop2 = tf.keras.layers.Dropout(0.5)\n",
    "    output_layer = tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "\n",
    "    model = tf.keras.Sequential([embedding_layer,conv1D,pool,flatten,drop1,dense_layer,drop2,output_layer])\n",
    "    return model\n",
    "\n",
    "tensprflow_model = tf.keras.Sequential([ tf.keras.layers.Embedding(420 + 1, 100),\n",
    "                                        tf.keras.layers.Dropout(0.2),\n",
    "                                        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "                                        tf.keras.layers.Dropout(0.2),\n",
    "                                        tf.keras.layers.Dense(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(2, 20), dtype=int64, numpy=\n",
      "array([[  2,   2,   3,   4,   5,  29,  30, 271,  28,  23,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0],\n",
      "       [  2,   2,   3,   4,   5,   7,   6,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0]])>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return tokenizer(text), label\n",
    "final_dataset = dataset.map(vectorize_text)\n",
    "for sample in final_dataset.take(1):\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size=vocab_size, embed_dim=10, Sequnce_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = tensorboard(model_name)\n",
    "f1_score = tf.keras.metrics.F1Score()\n",
    "tensprflow_model.compile(loss=loss, optimizer='adam', metrics= f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected scalar shape, saw shape: (1,).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tensprflow_model\u001b[39m.\u001b[39;49mfit(final_dataset, callbacks\u001b[39m=\u001b[39;49m[tensorboard_cb], batch_size\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "File \u001b[0;32m/workspace/log_anomaly/env/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/workspace/log_anomaly/env/lib/python3.11/site-packages/tensorboard/plugins/scalar/summary_v2.py:88\u001b[0m, in \u001b[0;36mscalar\u001b[0;34m(name, data, step, description)\u001b[0m\n\u001b[1;32m     83\u001b[0m summary_scope \u001b[39m=\u001b[39m (\n\u001b[1;32m     84\u001b[0m     \u001b[39mgetattr\u001b[39m(tf\u001b[39m.\u001b[39msummary\u001b[39m.\u001b[39mexperimental, \u001b[39m\"\u001b[39m\u001b[39msummary_scope\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     85\u001b[0m     \u001b[39mor\u001b[39;00m tf\u001b[39m.\u001b[39msummary\u001b[39m.\u001b[39msummary_scope\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     87\u001b[0m \u001b[39mwith\u001b[39;00m summary_scope(name, \u001b[39m\"\u001b[39m\u001b[39mscalar_summary\u001b[39m\u001b[39m\"\u001b[39m, values\u001b[39m=\u001b[39m[data, step]) \u001b[39mas\u001b[39;00m (tag, _):\n\u001b[0;32m---> 88\u001b[0m     tf\u001b[39m.\u001b[39;49mdebugging\u001b[39m.\u001b[39;49massert_scalar(data)\n\u001b[1;32m     89\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39msummary\u001b[39m.\u001b[39mwrite(\n\u001b[1;32m     90\u001b[0m         tag\u001b[39m=\u001b[39mtag,\n\u001b[1;32m     91\u001b[0m         tensor\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mcast(data, tf\u001b[39m.\u001b[39mfloat32),\n\u001b[1;32m     92\u001b[0m         step\u001b[39m=\u001b[39mstep,\n\u001b[1;32m     93\u001b[0m         metadata\u001b[39m=\u001b[39msummary_metadata,\n\u001b[1;32m     94\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected scalar shape, saw shape: (1,)."
     ]
    }
   ],
   "source": [
    "tensprflow_model.fit(final_dataset, callbacks=[tensorboard_cb], batch_size=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
